# åŠ è½½ESMåµŒå…¥å‘é‡
# è®¾ç½®éšæœºç§å­
import random
import os
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from time import time as t
from sklearn.metrics import mean_absolute_error, r2_score
from graphData import graphDataset
seed = 42
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# è®¾ç½®è®¾å¤‡
import random
import os
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from time import time as t
from sklearn.metrics import mean_absolute_error, r2_score
from graphData import graphDataset

# è®¾ç½®éšæœºç§å­
seed = 42
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# è®¾ç½®è®¾å¤‡
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# åŠ è½½å›¾æ•°æ®
gData = graphDataset("nature585BYamagata")
print("Node features (x) shape:", gData.data.x.shape)
print("Edge index shape:", gData.data.edge_index.shape)

data = gData.data
data.to(device)
data.edge_index = data.edge_index.to(device)
data.edge_attr = data.edge_attr.to(device)


# ========== å¤šæ¨¡å‹åµŒå…¥åŠ è½½å™¨ ==========
def load_embedding_from_csv(csv_path, model_type='auto'):
    """
    åŠ è½½ä¸åŒç±»å‹çš„åµŒå…¥å‘é‡

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        model_type: 'esm2', 'progen2', 'biogpt', 'auto'

    Returns:
        embeddings: torch.Tensor
        model_info: dict with model information
    """
    print(f"Loading embeddings from: {csv_path}")
    df = pd.read_csv(csv_path)

    # è‡ªåŠ¨æ£€æµ‹åµŒå…¥ç±»å‹
    embedding_prefixes = {
        'esm2': 'esm2_dim_',
        'progen2': 'progen2_dim_',
        'biogpt': 'biogpt_dim_'
    }

    detected_type = None
    embedding_cols = []

    if model_type == 'auto':
        # è‡ªåŠ¨æ£€æµ‹
        for prefix_name, prefix in embedding_prefixes.items():
            cols = [col for col in df.columns if col.startswith(prefix)]
            if cols:
                detected_type = prefix_name
                embedding_cols = cols
                break
    else:
        # æŒ‡å®šç±»å‹
        if model_type in embedding_prefixes:
            prefix = embedding_prefixes[model_type]
            embedding_cols = [col for col in df.columns if
                              col.startswith(prefix)]
            detected_type = model_type

    if not embedding_cols:
        raise ValueError(f"No embedding columns found for type: {model_type}")

    # æŒ‰æ•°å­—é¡ºåºæ’åº
    embedding_cols.sort(key=lambda x: int(x.split('_')[-1]))

    # æå–æ¨¡å‹ä¿¡æ¯
    model_info = {
        'type': detected_type,
        'embedding_dim': len(embedding_cols),
        'num_sequences': len(df)
    }

    if 'model' in df.columns:
        model_info['model_name'] = df['model'].iloc[0]
    if 'parameters' in df.columns:
        model_info['parameters'] = df['parameters'].iloc[0]
    if 'embedding_dim' in df.columns:
        model_info['reported_dim'] = df['embedding_dim'].iloc[0]

    # æå–åµŒå…¥å‘é‡
    embeddings = torch.tensor(df[embedding_cols].values, dtype=torch.float32)

    print(f"âœ… Detected model type: {detected_type}")
    print(f"ğŸ“Š Embedding shape: {embeddings.shape}")
    print(f"ğŸ”¢ Embedding dimension: {model_info['embedding_dim']}")
    if 'model_name' in model_info:
        print(f"ğŸ¤– Model: {model_info['model_name']}")
    if 'parameters' in model_info:
        print(f"âš™ï¸  Parameters: {model_info['parameters']}")

    return embeddings, model_info


# ========== é€‰æ‹©è¦ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹ ==========
print("\n=== Embedding Model Selection ===")

# å¯ç”¨çš„åµŒå…¥æ–‡ä»¶
available_embeddings = {
    # 'esm2': 'data/nature566H3N2_sequences_ESM2_embeddings.csv',
    # 'progen2': 'data/nature566H1N1_sequences_ProGen2-base_embeddings.csv',  # å‡è®¾è¿™æ˜¯æ‚¨çš„ProGen2æ–‡ä»¶
    # 'biogpt': 'data/nature566H3N2_sequences_BioGPT_embeddings.csv'  # å‡è®¾è¿™æ˜¯æ‚¨çš„BioGPTæ–‡ä»¶
    # 'esm2': 'data/nature585BVictoria_sequences_ESM2_embeddings.csv',
    # 'progen2': 'data/nature585BVictoria_sequences_ProGen2-base_embeddings.csv',  # å‡è®¾è¿™æ˜¯æ‚¨çš„ProGen2æ–‡ä»¶
    # 'biogpt': 'data/nature585BVictoria_sequences_BioGPT_embeddings.csv'  # å‡è®¾è¿™æ˜¯æ‚¨çš„BioGPTæ–‡ä»¶
    'esm2': 'data/nature585BYamagata_sequences_ESM2_embeddings.csv',
    'progen2': 'data/nature585BYamagata_sequences_ProGen2-base_embeddings.csv',  # å‡è®¾è¿™æ˜¯æ‚¨çš„ProGen2æ–‡ä»¶
    'biogpt': 'data/nature585BYamagata_sequences_BioGPT_embeddings.csv'
}

# è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹æˆ–è‡ªåŠ¨æ£€æµ‹
EMBEDDING_CHOICE = 'esm2'  # æ›´æ”¹è¿™é‡Œæ¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹ï¼š'esm2', 'progen2', 'biogpt'

print(f"ğŸ¯ Selected embedding model: {EMBEDDING_CHOICE}")

# æ ¹æ®é€‰æ‹©åŠ è½½ç›¸åº”çš„åµŒå…¥
if EMBEDDING_CHOICE in available_embeddings:
    csv_path = available_embeddings[EMBEDDING_CHOICE]
else:
    # å¦‚æœæŒ‡å®šäº†å…·ä½“æ–‡ä»¶è·¯å¾„
    csv_path = EMBEDDING_CHOICE

try:
    embeddings, model_info = load_embedding_from_csv(csv_path,
                                                     model_type='auto')
    embeddings = embeddings.to(device)

    print(f"\nğŸ“‹ Model Information:")
    for key, value in model_info.items():
        print(f"   {key}: {value}")

except FileNotFoundError:
    print(f"âŒ File not found: {csv_path}")
    print("Available options:")
    for name, path in available_embeddings.items():
        print(f"   {name}: {path}")
    raise

# éªŒè¯èŠ‚ç‚¹æ•°é‡åŒ¹é…
num_nodes = data.edge_index.max().item() + 1
print(f"\nğŸ” Validation:")
print(f"Graph nodes: {num_nodes}")
print(f"Embedding sequences: {embeddings.shape[0]}")

if embeddings.shape[0] != num_nodes:
    print("âš ï¸  Warning: Number of sequences doesn't match graph nodes")
    print("Adjusting embeddings...")

    if embeddings.shape[0] > num_nodes:
        embeddings = embeddings[:num_nodes]
        print(f"âœ‚ï¸  Truncated to {num_nodes} sequences")
    else:
        # å¦‚æœåµŒå…¥åºåˆ—å°‘äºå›¾èŠ‚ç‚¹ï¼Œç”¨é›¶å¡«å……æˆ–é‡å¤
        diff = num_nodes - embeddings.shape[0]
        padding = torch.zeros(diff, embeddings.shape[1], device=device)
        embeddings = torch.cat([embeddings, padding], dim=0)
        print(f"ğŸ“Œ Padded with {diff} zero vectors")

# ========== æ›¿æ¢å›¾æ•°æ®çš„ç‰¹å¾ ==========
data.x = embeddings
print(f"\nâœ… Updated data.x shape: {data.x.shape}")

# è·å–æœ€ç»ˆçš„åµŒå…¥ç»´åº¦
embedding_dim = data.x.shape[1]
print(f"ğŸ“Š Final embedding dimension: {embedding_dim}")
print(f"ğŸ§¬ Number of nodes: {data.x.shape[0]}")

# ========== è®­ç»ƒè§£ç å™¨é¢„æµ‹æŠ—åŸè·ç¦» ==========
print(
    f"\n=== Training Decoder with {model_info['type'].upper()} Embeddings ===")

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
from sklearn.model_selection import train_test_split
train_idx, test_idx = train_test_split(
    range(data.edge_index.shape[1]),
    test_size=0.2,
    random_state=42
)

train_edge_attr = data.edge_attr[train_idx]
test_edge_attr = data.edge_attr[test_idx]

print(f"Training edges: {len(train_idx)}")
print(f"Test edges: {len(test_idx)}")

# åˆå§‹åŒ–è§£ç å™¨
try:
    from model import GCNDecoder
    print("GCNDecoder imported successfully")
except ImportError as e:
    print(f"Import error: {e}")
    print("Please check if model.py exists and contains GCNDecoder class")

# ========== å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ESMåµŒå…¥ç»´åº¦ ==========
input_dim = embedding_dim  # 1280 (ESMåµŒå…¥ç»´åº¦)ï¼Œä¸å†æ˜¯num_hidden
out_feats = 512  # ä¸­é—´å±‚ç»´åº¦

print(f"Decoder input dimension: {input_dim}")
print(f"Decoder output features: {out_feats}")

decoder = GCNDecoder(input_dim=input_dim, out_feats=out_feats).to(device)
decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005, weight_decay=0.001)
scheduler = torch.optim.lr_scheduler.StepLR(decoder_optimizer, step_size=100, gamma=0.9)

# è®­ç»ƒè§£ç å™¨
print("\n=== Training Decoder with  Embeddings ===")
best_train_loss = float('inf')
start = t()

for epoch in range(1, 6001):  # 1500è½®è®­ç»ƒ
    decoder.train()
    decoder_optimizer.zero_grad()

    # é¢„æµ‹è®­ç»ƒé›†è¾¹å±æ€§
    predicted_edge_attr = decoder(data.x, data.edge_index[:, train_idx])
    train_loss = F.mse_loss(predicted_edge_attr, train_edge_attr)

    train_loss.backward()
    decoder_optimizer.step()
    scheduler.step()

    # æ¯50è½®è¯„ä¼°ä¸€æ¬¡
    if epoch % 50 == 0:
        decoder.eval()
        with torch.no_grad():
            # æµ‹è¯•é›†è¯„ä¼°
            test_predicted = decoder(data.x, data.edge_index[:, test_idx])
            test_loss = F.mse_loss(test_predicted, test_edge_attr)
            test_mae = mean_absolute_error(test_edge_attr.cpu().numpy(),
                                         test_predicted.cpu().numpy())
            test_r2 = r2_score(test_edge_attr.cpu().numpy(),
                             test_predicted.cpu().numpy())

        now = t()
        print(f'Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | '
              f'Test Loss: {test_loss:.4f} | MAE: {test_mae:.4f} | '
              f'RÂ²: {test_r2:.4f} | Time: {now - start:.2f}s')

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if train_loss < best_train_loss:
            best_train_loss = train_loss
            torch.save(decoder.state_dict(), 'best_esm_decoder.pth')

# æœ€ç»ˆè¯„ä¼°
print("\n=== Final Evaluation with ESM Embeddings ===")
decoder.eval()
with torch.no_grad():
    test_predicted = decoder(data.x, data.edge_index[:, test_idx])
    test_loss = F.mse_loss(test_predicted, test_edge_attr)
    test_mae = mean_absolute_error(test_edge_attr.cpu().numpy(),
                                 test_predicted.cpu().numpy())
    test_r2 = r2_score(test_edge_attr.cpu().numpy(),
                     test_predicted.cpu().numpy())

print("\n=== Final Results with ESM Embeddings ===")
print(f'Test MSE: {test_loss:.4f}')
print(f'Test MAE: {test_mae:.4f}')
print(f'Test RÂ²: {test_r2:.4f}')

# é¢å¤–åˆ†æï¼šæ¯”è¾ƒçœŸå®å€¼å’Œé¢„æµ‹å€¼
print(f"\n=== Prediction Analysis ===")
test_real = test_edge_attr.cpu().numpy()
test_pred = test_predicted.cpu().numpy()

print(f"Real values range: [{test_real.min():.4f}, {test_real.max():.4f}]")
print(f"Predicted values range: [{test_pred.min():.4f}, {test_pred.max():.4f}]")
print(f"Mean absolute difference: {np.mean(np.abs(test_real - test_pred)):.4f}")

# ä¿å­˜æœ€ç»ˆç»“æœ
results_df = pd.DataFrame({
    'real_distance': test_real.flatten(),
    'predicted_distance': test_pred.flatten(),
    'absolute_error': np.abs(test_real - test_pred).flatten()
})

# results_df.to_csv('esm_decoder_predictions.csv', index=False)
# print("Prediction results saved to 'esm_decoder_predictions.csv'")

# print(f"\nğŸ‰ ESMåµŒå…¥å‘é‡æŠ—åŸè·ç¦»é¢„æµ‹å®Œæˆï¼")
# print(f"ğŸ“Š ä½¿ç”¨äº†1280ç»´ESMåµŒå…¥æ›¿ä»£åŸå§‹ç‰¹å¾")
print(f"ğŸ”¬ è§£ç å™¨æˆåŠŸé¢„æµ‹æŠ—åŸè·ç¦»")